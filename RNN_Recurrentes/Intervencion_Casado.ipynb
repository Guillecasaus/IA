{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAQ4EFQn66Q"
      },
      "source": [
        "**Importación de Librerias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lPpBIgIAAIxp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ifaDLnBICO",
        "outputId": "c50d4056-d8cf-45c8-fa73-ece0c7c010c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_04tdU-oCRj"
      },
      "source": [
        "**Descarga y preprocesado de datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u75nLCa-AW7m",
        "outputId": "11f7fea9-e6e2-4bda-9939-d9185968540c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto:        105940 carácteres\n",
            "El texto está compuesto de estos 92 carácteres:\n",
            "['\\n', ' ', '!', '%', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'Á', 'á', 'é', 'í', 'ñ', 'ó', 'ö', 'ú', 'ü', '\\u2009', '\\u200c', '—', '―', '‘', '’', '…']\n"
          ]
        }
      ],
      "source": [
        "#Cargar el texto del parlamentario1\n",
        "text = open(\"/content/intervencionesCasado.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "print('Longitud del texto:        {} carácteres'.format(len(text)))\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "print ('El texto está compuesto de estos {} carácteres:'.format(len(vocab)))\n",
        "print (vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ItsMgJVBMC3",
        "outputId": "a5067650-d94e-456c-d190-a1b47fec544a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '%' :   3,\n",
            "  ',' :   4,\n",
            "  '-' :   5,\n",
            "  '.' :   6,\n",
            "  '0' :   7,\n",
            "  '1' :   8,\n",
            "  '2' :   9,\n",
            "  '3' :  10,\n",
            "  '4' :  11,\n",
            "  '5' :  12,\n",
            "  '6' :  13,\n",
            "  '7' :  14,\n",
            "  '8' :  15,\n",
            "  '9' :  16,\n",
            "  ':' :  17,\n",
            "  ';' :  18,\n",
            "  '?' :  19,\n",
            "  'A' :  20,\n",
            "  'B' :  21,\n",
            "  'C' :  22,\n",
            "  'D' :  23,\n",
            "  'E' :  24,\n",
            "  'F' :  25,\n",
            "  'G' :  26,\n",
            "  'H' :  27,\n",
            "  'I' :  28,\n",
            "  'J' :  29,\n",
            "  'L' :  30,\n",
            "  'M' :  31,\n",
            "  'N' :  32,\n",
            "  'O' :  33,\n",
            "  'P' :  34,\n",
            "  'Q' :  35,\n",
            "  'R' :  36,\n",
            "  'S' :  37,\n",
            "  'T' :  38,\n",
            "  'U' :  39,\n",
            "  'V' :  40,\n",
            "  'X' :  41,\n",
            "  'Y' :  42,\n",
            "  'Z' :  43,\n",
            "  '[' :  44,\n",
            "  ']' :  45,\n",
            "  'a' :  46,\n",
            "  'b' :  47,\n",
            "  'c' :  48,\n",
            "  'd' :  49,\n",
            "  'e' :  50,\n",
            "  'f' :  51,\n",
            "  'g' :  52,\n",
            "  'h' :  53,\n",
            "  'i' :  54,\n",
            "  'j' :  55,\n",
            "  'k' :  56,\n",
            "  'l' :  57,\n",
            "  'm' :  58,\n",
            "  'n' :  59,\n",
            "  'o' :  60,\n",
            "  'p' :  61,\n",
            "  'q' :  62,\n",
            "  'r' :  63,\n",
            "  's' :  64,\n",
            "  't' :  65,\n",
            "  'u' :  66,\n",
            "  'v' :  67,\n",
            "  'w' :  68,\n",
            "  'x' :  69,\n",
            "  'y' :  70,\n",
            "  'z' :  71,\n",
            "  '¡' :  72,\n",
            "  '«' :  73,\n",
            "  '»' :  74,\n",
            "  '¿' :  75,\n",
            "  'Á' :  76,\n",
            "  'á' :  77,\n",
            "  'é' :  78,\n",
            "  'í' :  79,\n",
            "  'ñ' :  80,\n",
            "  'ó' :  81,\n",
            "  'ö' :  82,\n",
            "  'ú' :  83,\n",
            "  'ü' :  84,\n",
            "  '\\u2009':  85,\n",
            "  '\\u200c':  86,\n",
            "  '—' :  87,\n",
            "  '―' :  88,\n",
            "  '‘' :  89,\n",
            "  '’' :  90,\n",
            "  '…' :  91,\n"
          ]
        }
      ],
      "source": [
        "#Como las redes neuronales solo procesan valores numéricos, no letras. Traduciremos los caracteres a representación numérica.\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "for char,_ in zip(char2idx, range(len(vocab))):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e-7yufZABUlU"
      },
      "outputs": [],
      "source": [
        "#Ahora pasaremos el texto a un array de enteros\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wq-zN5kBVKK",
        "outputId": "af393ac0-0a58-4dfd-8b5f-c8a7d70a6389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texto : 'Señor Sánchez, sus recetas económicas son tan creí'\n",
            "array([37, 50, 80, 60, 63,  1, 37, 77, 59, 48, 53, 50, 71,  4,  1, 64, 66,\n",
            "       64,  1, 63, 50, 48, 50, 65, 46, 64,  1, 50, 48, 60, 59, 81, 58, 54,\n",
            "       48, 46, 64,  1, 64, 60, 59,  1, 65, 46, 59,  1, 48, 63, 50, 79])\n"
          ]
        }
      ],
      "source": [
        "#Mostramos los 50 primeros caracteres del texto text_as_init\n",
        "print ('texto : {}'.format(repr(text[:50])))\n",
        "print ('{}'.format(repr(text_as_int[:50])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiEo1IbooZT9"
      },
      "source": [
        "**Preparación de los datos para entrenar la RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s5q3NUFIBVOt"
      },
      "outputs": [],
      "source": [
        "#Para entrenar el modelo creamos un conjunto de datos con el contenido de text_as_init. Para ello utilizamos la función tf.data.Dataset.from_tensor_slices.\n",
        "#A este conjunto de datos lo dividiremos en secuencias de seq_length+1 al aplicar el método batch()\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "seq_length = 100\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_04GR_o-BVVb",
        "outputId": "4cfd8b9a-716c-40ec-9fc9-f9b1a3e50734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Señor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima propon'\n",
            "'en las mismas recetas fracasadas que nos llevaron a la peor crisis económica de nuestra historia: más'\n",
            "' despilfarro, más déficit y más impuestos. Pero el Partido Popular es un partido de Estado y también '\n",
            "'de Gobierno, aunque estemos temporalmente en la oposición. Por eso el lunes le ofrecí pactar los Pres'\n",
            "'upuestos Generales si rompe con los independentistas, una oferta, por cierto, a la que usted no ha co'\n",
            "'ntestado. Hoy tiene oportunidad de hacerlo, porque ya no tiene excusa. Usted eligió libremente a sus '\n",
            "'socios para la investidura, pero si ahora elige otra vez a los mismos para los presupuestos es porque'\n",
            "' quiere. ¿O cree que al señor Torra le importan los parados en España, o al señor Otegi los agriculto'\n",
            "'res y ganaderos, o al señor Junqueras los pensionistas o los autónomos? Ya lo dijeron aquí, les impor'\n",
            "'ta España un comino. Y parece que a usted también si acepta una mesa de autodeterminación con un inha'\n"
          ]
        }
      ],
      "source": [
        "#Comprobamos que las sequences son el texto dividido en 101 caracteres (mostramos 10 secuencias)\n",
        "for item in sequences.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5k9qphFpB5ws"
      },
      "outputs": [],
      "source": [
        "#Creamos una función que devolverá el conjunto de datos de entrenamiento (los datos de entrada como los datos de salida)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "#Aplicamos la función a todas las secuencias utilizando el método map()\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfs-DM9XB56u",
        "outputId": "1e856664-f6cc-416a-aee8-6bc6f2b0d248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'Señor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima propo'\n",
            "Target data: 'eñor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima propon'\n",
            "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "#Los dataset contienen un conjunto de parejas (100 caracteres del texto original, la correspondiente salida ). Vamos a mostrar la primera pareja.\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n",
        "  print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G69aZMOIB6AM",
        "outputId": "7b09df0c-2e05-43ed-e269-9c477773233b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "#Agrupamos los dataset en batches de 64 . Así tendriamos los datos de entrenamiento con batches compuestos de 64 parejas de secuencias de 100 integers de 64 bits\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print (dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv8SzZY9o-4G"
      },
      "source": [
        "**Construcción del modelo RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eB5oYVSUB6EZ"
      },
      "outputs": [],
      "source": [
        "#Crearemos una función que cree un modelo RNN con tres capas\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = Sequential()\n",
        "  #Añadimos la capa de tipo word embedding\n",
        "  model.add(Embedding(input_dim=vocab_size,\n",
        "                      output_dim=embedding_dim,\n",
        "                      #batch_input_shape=[batch_size, None] Deprecated\n",
        "                      ))\n",
        "  #Añadimos la capa de tipo LSTM\n",
        "  model.add(LSTM(rnn_units,\n",
        "                 return_sequences=True,\n",
        "                 stateful=True,\n",
        "                 recurrent_initializer='glorot_uniform'))\n",
        "  model.add(Dense(512, activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "  #Añadimos la capa de tipo Dense\n",
        "  model.add(Dense(vocab_size))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YdgPsCk2CUiA"
      },
      "outputs": [],
      "source": [
        "#Creamos el modelo\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "rl8nlYQsCUrn",
        "outputId": "d2e55551-7085-44be-8270-914b0ffcf70d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZlHgoPCUwY",
        "outputId": "d2a89fcc-da53-487c-d1ee-d64a15b5cd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (64, 100) # (batch_size, sequence_length)\n",
            "Target: (64, 100) # (batch_size, sequence_length)\n"
          ]
        }
      ],
      "source": [
        "#A continuación inspecionamos las dimenciones de los tensores\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjbTgZS2CUzO",
        "outputId": "be827ead-5b9a-413b-901b-e5cd3332e37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction :  (64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\"Prediction : \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y7gp_axeCpnI"
      },
      "outputs": [],
      "source": [
        "#Obtenemos una muestra de la distribución de salida\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L8mDKIdCpp_",
        "outputId": "07b83d6f-152e-4d81-9932-244216cb11e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55 22 67 20 44 67 25 43  4 78 52 83 70 19 43 78 10 16 42 33 11 61  2 78\n",
            " 64 42 17 37 45  7 10 81 54 35 38  6 65 37  0 81 27 88 75 26 56 10  4 90\n",
            " 60 23 42  0 43 32 71 20 75 23 66 35 38 14 36 46 74 18 49  7 67 59 10 53\n",
            " 79 23 41 73 35  8 50 86  9 84 70 91 91 22 79 38 67 22 52 87 49 82 27 54\n",
            " 76 23 41 47]\n"
          ]
        }
      ],
      "source": [
        "print(sampled_indices_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvq7sCMpNWS"
      },
      "source": [
        "**Entrenamiento del modelo RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uKqSJOP_Cpso"
      },
      "outputs": [],
      "source": [
        "#Creamos la función de perdida, usaremos el categorical pues estamos considerando datos categóricos\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YPNOYQq9Cpu-"
      },
      "outputs": [],
      "source": [
        "#Compilamos el modelo\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qqxhp8m-Cpw1"
      },
      "outputs": [],
      "source": [
        "#configuramos los checkpoints\n",
        "\n",
        "checkpoint_dir = './training_checkpoints_Casado'\n",
        "\n",
        "# nombre fichero\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(reference_text, generated_text):\n",
        "\n",
        "    \"\"\"\n",
        "    Calcula el BLEU score comparando el texto generado con el texto de referencia.\n",
        "    \"\"\"\n",
        "\n",
        "    reference_tokens = [list(reference_text)]\n",
        "    generated_tokens = list(generated_text)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "    score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smooth_fn)\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "def calculate_perplexity(predictions):\n",
        "    \"\"\"\n",
        "    Calcula la perplejidad promedio de una secuencia basada en los logits de salida del modelo.\n",
        "    \"\"\"\n",
        "    # Convertimos logits a probabilidades con softmax\n",
        "    probs = tf.nn.softmax(predictions, axis=-1).numpy()\n",
        "\n",
        "    # Obtenemos las probabilidades máximas (las más probables)\n",
        "    max_probs = np.max(probs, axis=-1)\n",
        "\n",
        "    # Calculamos el logaritmo negativo de las probabilidades\n",
        "    log_probs = -np.log(max_probs + 1e-10)  # Evitamos log(0) con 1e-10\n",
        "\n",
        "    # Calculamos la perplejidad: exponencial de la media de los logaritmos\n",
        "    perplexity = np.exp(np.mean(log_probs))\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "\n",
        "def compare_word_distribution(reference_text, generated_text):\n",
        "\n",
        "    \"\"\"\n",
        "    Compara la distribución de palabras entre el texto generado y el de referencia.\n",
        "    \"\"\"\n",
        "\n",
        "    ref_counter = Counter(reference_text.split())\n",
        "    gen_counter = Counter(generated_text.split())\n",
        "    total_ref = sum(ref_counter.values())\n",
        "    total_gen = sum(gen_counter.values())\n",
        "\n",
        "    ref_dist = {word: count / total_ref for word, count in ref_counter.items()}\n",
        "    gen_dist = {word: count / total_gen for word, count in gen_counter.items()}\n",
        "\n",
        "\n",
        "\n",
        "    kl_divergence = 0\n",
        "\n",
        "    for word, prob in gen_dist.items():\n",
        "        if word in ref_dist:\n",
        "            kl_divergence += prob * math.log(prob / ref_dist[word])\n",
        "\n",
        "    return kl_divergence\n"
      ],
      "metadata": {
        "id": "L0ruJPb-Bb6Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SROaEjLNDCuF",
        "outputId": "efeb7173-765a-4f7b-f235-13054c5a2293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 100ms/step - loss: 3.8592\n",
            "Epoch 2/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 3.0758\n",
            "Epoch 3/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2.7287\n",
            "Epoch 4/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 2.5688\n",
            "Epoch 5/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 2.4495\n",
            "Epoch 6/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2.3656\n",
            "Epoch 7/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 2.2786\n",
            "Epoch 8/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2.1890\n",
            "Epoch 9/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2.1140\n",
            "Epoch 10/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2.0448\n",
            "Epoch 11/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 1.9717\n",
            "Epoch 12/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 1.9025\n",
            "Epoch 13/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 1.8419\n",
            "Epoch 14/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 1.7559\n",
            "Epoch 15/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 1.7111\n",
            "Epoch 16/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 1.6587\n",
            "Epoch 17/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1.6055\n",
            "Epoch 18/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1.5541\n",
            "Epoch 19/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 1.5034\n",
            "Epoch 20/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1.4611\n",
            "Epoch 21/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 1.4072\n",
            "Epoch 22/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 1.3692\n",
            "Epoch 23/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1.3297\n",
            "Epoch 24/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 1.2910\n",
            "Epoch 25/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 1.2512\n",
            "Epoch 26/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 1.2054\n",
            "Epoch 27/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 1.1725\n",
            "Epoch 28/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1.1379\n",
            "Epoch 29/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 1.0949\n",
            "Epoch 30/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 1.0568\n",
            "Epoch 31/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 1.0211\n",
            "Epoch 32/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.9920\n",
            "Epoch 33/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 0.9545\n",
            "Epoch 34/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.9183\n",
            "Epoch 35/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 0.8960\n",
            "Epoch 36/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.8527\n",
            "Epoch 37/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.8143\n",
            "Epoch 38/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.7758\n",
            "Epoch 39/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.7483\n",
            "Epoch 40/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.7149\n",
            "Epoch 41/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.6768\n",
            "Epoch 42/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 0.6401\n",
            "Epoch 43/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - loss: 0.6131\n",
            "Epoch 44/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.5813\n",
            "Epoch 45/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.5465\n",
            "Epoch 46/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.5136\n",
            "Epoch 47/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.4844\n",
            "Epoch 48/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.4656\n",
            "Epoch 49/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 0.4468\n",
            "Epoch 50/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.4225\n",
            "Epoch 51/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.4023\n",
            "Epoch 52/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.3847\n",
            "Epoch 53/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.3695\n",
            "Epoch 54/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.3566\n",
            "Epoch 55/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 0.3393\n",
            "Epoch 56/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.3274\n",
            "Epoch 57/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.3189\n",
            "Epoch 58/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.3048\n",
            "Epoch 59/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.2953\n",
            "Epoch 60/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.2892\n",
            "Epoch 61/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 0.2762\n",
            "Epoch 62/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 0.2735\n",
            "Epoch 63/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.2665\n",
            "Epoch 64/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.2614\n",
            "Epoch 65/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.2537\n",
            "Epoch 66/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.2471\n",
            "Epoch 67/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.2432\n",
            "Epoch 68/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.2326\n",
            "Epoch 69/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.2272\n",
            "Epoch 70/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.2280\n",
            "Epoch 71/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.2270\n",
            "Epoch 72/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.2195\n",
            "Epoch 73/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.2154\n",
            "Epoch 74/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.2127\n",
            "Epoch 75/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.2114\n",
            "Epoch 76/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.2084\n",
            "Epoch 77/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.2059\n",
            "Epoch 78/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 0.2008\n",
            "Epoch 79/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1963\n",
            "Epoch 80/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1967\n",
            "Epoch 81/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 0.1930\n",
            "Epoch 82/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1907\n",
            "Epoch 83/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1905\n",
            "Epoch 84/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 0.1876\n",
            "Epoch 85/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.1866\n",
            "Epoch 86/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1831\n",
            "Epoch 87/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1810\n",
            "Epoch 88/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1762\n",
            "Epoch 89/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1783\n",
            "Epoch 90/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.1722\n",
            "Epoch 91/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - loss: 0.1745\n",
            "Epoch 92/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1697\n",
            "Epoch 93/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1729\n",
            "Epoch 94/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.1650\n",
            "Epoch 95/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1675\n",
            "Epoch 96/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1678\n",
            "Epoch 97/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.1648\n",
            "Epoch 98/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.1637\n",
            "Epoch 99/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1596\n",
            "Epoch 100/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1582\n",
            "Epoch 101/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1619\n",
            "Epoch 102/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1590\n",
            "Epoch 103/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.1558\n",
            "Epoch 104/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 0.1572\n",
            "Epoch 105/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1554\n",
            "Epoch 106/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.1549\n",
            "Epoch 107/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 0.1531\n",
            "Epoch 108/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.1518\n",
            "Epoch 109/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1534\n",
            "Epoch 110/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.1501\n",
            "Epoch 111/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 0.1508\n",
            "Epoch 112/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1483\n",
            "Epoch 113/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1523\n",
            "Epoch 114/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1508\n",
            "Epoch 115/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.1488\n",
            "Epoch 116/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1497\n",
            "Epoch 117/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.1492\n",
            "Epoch 118/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 0.1421\n",
            "Epoch 119/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1424\n",
            "Epoch 120/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1415\n",
            "Epoch 121/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.1392\n",
            "Epoch 122/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.1409\n",
            "Epoch 123/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.1379\n",
            "Epoch 124/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1366\n",
            "Epoch 125/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1358\n",
            "Epoch 126/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1394\n",
            "Epoch 127/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1393\n",
            "Epoch 128/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1335\n",
            "Epoch 129/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.1372\n",
            "Epoch 130/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 0.1370\n",
            "Epoch 131/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1367\n",
            "Epoch 132/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1391\n",
            "Epoch 133/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 0.1326\n",
            "Epoch 134/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1314\n",
            "Epoch 135/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1344\n",
            "Epoch 136/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - loss: 0.1325\n",
            "Epoch 137/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1288\n",
            "Epoch 138/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1324\n",
            "Epoch 139/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1324\n",
            "Epoch 140/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1316\n",
            "Epoch 141/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1336\n",
            "Epoch 142/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - loss: 0.1316\n",
            "Epoch 143/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1256\n",
            "Epoch 144/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1285\n",
            "Epoch 145/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1272\n",
            "Epoch 146/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1282\n",
            "Epoch 147/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1249\n",
            "Epoch 148/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 0.1250\n",
            "Epoch 149/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.1226\n",
            "Epoch 150/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1231\n",
            "Epoch 151/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1245\n",
            "Epoch 152/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1191\n",
            "Epoch 153/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 0.1227\n",
            "Epoch 154/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1212\n",
            "Epoch 155/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 0.1217\n",
            "Epoch 156/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.1201\n",
            "Epoch 157/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1224\n",
            "Epoch 158/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1190\n",
            "Epoch 159/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1185\n",
            "Epoch 160/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1180\n",
            "Epoch 161/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1197\n",
            "Epoch 162/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.1175\n",
            "Epoch 163/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 0.1177\n",
            "Epoch 164/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1208\n",
            "Epoch 165/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1238\n",
            "Epoch 166/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.1215\n",
            "Epoch 167/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.1124\n",
            "Epoch 168/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.1188\n",
            "Epoch 169/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1190\n",
            "Epoch 170/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1179\n",
            "Epoch 171/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.1166\n",
            "Epoch 172/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1141\n",
            "Epoch 173/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1157\n",
            "Epoch 174/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1164\n",
            "Epoch 175/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 0.1169\n",
            "Epoch 176/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.1143\n",
            "Epoch 177/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 0.1148\n",
            "Epoch 178/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1126\n",
            "Epoch 179/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1121\n",
            "Epoch 180/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 0.1120\n",
            "Epoch 181/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1113\n",
            "Epoch 182/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1116\n",
            "Epoch 183/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1140\n",
            "Epoch 184/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1114\n",
            "Epoch 185/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1093\n",
            "Epoch 186/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1136\n",
            "Epoch 187/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.1103\n",
            "Epoch 188/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - loss: 0.1119\n",
            "Epoch 189/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.1127\n",
            "Epoch 190/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1111\n",
            "Epoch 191/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1098\n",
            "Epoch 192/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1112\n",
            "Epoch 193/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1100\n",
            "Epoch 194/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.1105\n",
            "Epoch 195/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 0.1086\n",
            "Epoch 196/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1082\n",
            "Epoch 197/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1091\n",
            "Epoch 198/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - loss: 0.1070\n",
            "Epoch 199/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1082\n",
            "Epoch 200/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.1064\n",
            "Epoch 201/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.1067\n",
            "Epoch 202/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1069\n",
            "Epoch 203/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1058\n",
            "Epoch 204/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1086\n",
            "Epoch 205/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.1064\n",
            "Epoch 206/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.1017\n",
            "Epoch 207/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1063\n",
            "Epoch 208/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.1041\n",
            "Epoch 209/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1038\n",
            "Epoch 210/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.1047\n",
            "Epoch 211/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1035\n",
            "Epoch 212/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 0.1038\n",
            "Epoch 213/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 0.1048\n",
            "Epoch 214/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1053\n",
            "Epoch 215/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1038\n",
            "Epoch 216/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.1026\n",
            "Epoch 217/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.1024\n",
            "Epoch 218/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1013\n",
            "Epoch 219/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.1005\n",
            "Epoch 220/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1050\n",
            "Epoch 221/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 0.1046\n",
            "Epoch 222/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1021\n",
            "Epoch 223/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 0.1028\n",
            "Epoch 224/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.1046\n",
            "Epoch 225/225\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.1035\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos el modelo\n",
        "EPOCHS=225\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UwLoIf-eJNDH"
      },
      "outputs": [],
      "source": [
        "model.save(\"model_Casado.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Mk5PTDmwAqsJ"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from keras import losses # Import the losses module\n",
        "\n",
        "# Assuming your original loss function was, for example, binary_crossentropy\n",
        "loaded_model = load_model(\"model_Casado.keras\",\n",
        "                          custom_objects={'loss': losses.sparse_categorical_crossentropy})\n",
        "# or if it was a custom loss function\n",
        "# loaded_model = load_model(\"model_paquita_100_2024.keras\", custom_objects={'loss': my_custom_loss_function})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3OOgx6ipdGl"
      },
      "source": [
        "**Generación de texto del modelo RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nGlNTy7xDCw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f886bc-09cb-4a61-83ad-b9dded8334b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplejidad: 1.781034231185913\n"
          ]
        }
      ],
      "source": [
        "#Ya que tenemos entrenado los modelos vamos a usarlos para generar texto\n",
        "#Vamos a reconstruir manualmente los modelos para modifical el batch y su peso para poner el último\n",
        "model = build_model(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
        "# Build the model before loading weights\n",
        "# This creates the necessary layer variables to which weights can be assigned\n",
        "# Provide an input shape that matches the expected input of your model\n",
        "# for example:\n",
        "# input_shape = (1, sequence_length) where sequence_length is the length of your input sequences\n",
        "input_shape = (1, 100)  # Replace 100 with your actual sequence length\n",
        "model.build(input_shape=input_shape) # Or model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.load_weights(\"model_Casado.keras\")\n",
        "\n",
        "# Genera logits para una entrada de prueba\n",
        "sample_input = tf.expand_dims([char2idx[s] for s in \"texto de prueba\"], axis=0)\n",
        "logits = model(sample_input)  # Salida sin aplicar softmax\n",
        "\n",
        "# Calcula la perplejidad con la función corregida\n",
        "perplexity_value = calculate_perplexity(logits)\n",
        "print(f\"Perplejidad: {perplexity_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "q05VTagxDCzG"
      },
      "outputs": [],
      "source": [
        "#Creamos una función generar_texto que generará texto a partir de una palabra de partida\n",
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 1000\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "\n",
        "\n",
        "  temperature = 0.45\n",
        "\n",
        "#  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rac2ypCzDC1b",
        "outputId": "2137efa3-3486-49da-fe7a-92a78fd3b3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoy en el pleno en la legislación en vigor. .\n",
            "Gracias, señora presidenta. Antes se ha referido a algo parecido a la oposición, a la que trata a patadas. Usted cree que este es el escenario y el tablero de ajedrenti una nefasta política de compra del material sanitario. Somos también, según el FMI, la segunda econtes, a las fallecidos, alcanzaría la nueva moralidad. Mire a su alrededor, no les decira, disuelva el Parlamento y convoque elecciones. . Solo así podrá quizá indultarle la historia. Muchas gracias. .\n",
            "Dice que no deja a nadie atrás, pero no estamos en la solidaridad interterritorial y de la igualdad entre españoles, y acaban con el currículo educativo con implementación a nivel nacional de un porcentaje mínimo, digo que lo debería costar el cargo a su fiscalía por habitante del mundo, 18 000; y somos el país con más sanitarios contagiados del mundo por culpa de la arrogancia y la incompetencia de Sánchez, pero también por sus personas de los españoles. Muchas gracias. .\n",
            "Señor Sánchez, después\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, start_string=u\"Hoy en el pleno\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnPV4P_dLFt9",
        "outputId": "fade9838-2455-429b-902a-f45e93fe6262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Es inaceptable que este gobierno haya decidido no intervenir. No interviene el presidente del Gobierno, pues no intervenimos nosotros. El primero, el Pacto Cajal por la sanidad, dentro de la comisión parlamentaria, que yo le propuse y, por tanto, la responsabilidad es suya. Lealtad no es patente de corso; unidad no es barra libre y no hay que reformarla, basta con mundo más españoles. .\n",
            "Señor Sánchez, no nos haga comulga de la Constitución Gabriel Cisneros. ¿Va usted a sacar de la cárcel a doscientos terroristas para comprar videoja de todos los indicadores negativos a nivel internacional. Se lo digo sin parafrasearle, para que no se repitan cierres como los de Alcoa o Nissan y una comisión con autonomía de para que no se repitan cierres como los de Alcoa o Nissan y una comisión con autonomía de para destruir el diálogo por antonomasia que es nuestra Constitución. Ya dialogamos, ya nos reconciler los cuatro ha fallado. El primero era acabar con el paro, pero lo ha disparado a 5 millones. No es campo se le pide mucho. La Transición\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, start_string=u\"Es inaceptable que este gobierno haya decidido\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jaP2v8hkDC50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af30571-a8ef-415f-d1af-aa014d8d6b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.5312040881919403\n",
            "Perplejidad: 1.9012998342514038\n",
            "KL Divergence (Distribución de Palabras): 0.0872636562408613\n"
          ]
        }
      ],
      "source": [
        "# Texto de referencia: toma un segmento del dataset original\n",
        "reference_text = text[:1000]  # Ejemplo: primeros 1000 caracteres del texto original\n",
        "generated_text = generate_text(model, start_string=\"Ejemplo de inicio\")\n",
        "\n",
        "# BLEU Score\n",
        "bleu_score = calculate_bleu(reference_text, generated_text)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "\n",
        "# Perplejidad\n",
        "input_eval = [char2idx[s] for s in \"Ejemplo de inicio\"]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "predictions = model(input_eval)\n",
        "\n",
        "perplexity = calculate_perplexity(predictions[0])\n",
        "print(f\"Perplejidad: {perplexity}\")\n",
        "\n",
        "# Comparación de Distribución de Palabras\n",
        "kl_divergence = compare_word_distribution(reference_text, generated_text)\n",
        "print(f\"KL Divergence (Distribución de Palabras): {kl_divergence}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}